{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9bfs-H8Hv-5",
        "outputId": "9430bcda-54b8-420c-ee8d-76a4a8b139dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/twitter-entity-sentiment-analysis\n",
            "Files in dataset: ['twitter_validation.csv', 'twitter_training.csv']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import re\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN,Bidirectional, LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils import class_weight # Import class_weight from sklearn.utils\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "files = os.listdir(path)\n",
        "\n",
        "\n",
        "print(\"Files in dataset:\", files)\n",
        "data_file = os.path.join(path, \"twitter_training.csv\")\n",
        "train_df = pd.read_csv(data_file)\n",
        "data_file = os.path.join(path, \"twitter_validation.csv\")\n",
        "val_df = pd.read_csv(data_file)\n",
        "\n",
        "train_df.columns = ['Column1', 'Column2', 'Column3', 'Column4']\n",
        "val_df.columns = ['Column1', 'Column2', 'Column3', 'Column4']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Convert to string if not already\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower().strip()  # Convert to lowercase and strip whitespace\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Clean the datasets\n",
        "train_df['cleaned_text'] = train_df['Column4'].apply(clean_text)\n",
        "val_df['cleaned_text'] = val_df['Column4'].apply(clean_text)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['label_encoded'] = label_encoder.fit_transform(train_df['Column3'])\n",
        "val_df['label_encoded'] = label_encoder.transform(val_df['Column3'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "max_vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_df['cleaned_text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_df['cleaned_text'])\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "y_train = train_df['label_encoded']\n",
        "\n",
        "X_val = tokenizer.texts_to_sequences(val_df['cleaned_text'])\n",
        "X_val = pad_sequences(X_val, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "y_val = val_df['label_encoded']\n",
        "\n",
        "# Handle class imbalance\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_vocab_size, output_dim=128, input_length=max_sequence_length),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(128, activation='relu', kernel_regularizer='l2'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_reducer, early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = np.argmax(model.predict(X_val), axis=-1)\n",
        "\n",
        "precision = precision_score(y_val, y_pred, average='weighted')\n",
        "recall = recall_score(y_val, y_pred, average='weighted')\n",
        "f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(classification_report(y_val, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl9s6mKMH0PI",
        "outputId": "5018669d-c8c8-4343-ccdb-3a8fc0449020"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 29ms/step - accuracy: 0.5111 - loss: 1.2443 - val_accuracy: 0.8378 - val_loss: 0.5246 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 28ms/step - accuracy: 0.7939 - loss: 0.5931 - val_accuracy: 0.8879 - val_loss: 0.3312 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 29ms/step - accuracy: 0.8529 - loss: 0.4125 - val_accuracy: 0.9239 - val_loss: 0.2394 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 29ms/step - accuracy: 0.8831 - loss: 0.3207 - val_accuracy: 0.9329 - val_loss: 0.2200 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 28ms/step - accuracy: 0.9018 - loss: 0.2690 - val_accuracy: 0.9419 - val_loss: 0.2086 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 28ms/step - accuracy: 0.9169 - loss: 0.2255 - val_accuracy: 0.9459 - val_loss: 0.2286 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 28ms/step - accuracy: 0.9247 - loss: 0.2042 - val_accuracy: 0.9630 - val_loss: 0.1692 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 28ms/step - accuracy: 0.9278 - loss: 0.1855 - val_accuracy: 0.9520 - val_loss: 0.1853 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m2333/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9323 - loss: 0.1766\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 29ms/step - accuracy: 0.9323 - loss: 0.1766 - val_accuracy: 0.9479 - val_loss: 0.2157 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 29ms/step - accuracy: 0.9415 - loss: 0.1465 - val_accuracy: 0.9560 - val_loss: 0.1877 - learning_rate: 5.0000e-04\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Precision: 0.96\n",
            "Recall: 0.96\n",
            "F1 Score: 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.97      0.95      0.96       171\n",
            "    Negative       0.96      0.96      0.96       266\n",
            "     Neutral       0.96      0.97      0.97       285\n",
            "    Positive       0.96      0.96      0.96       277\n",
            "\n",
            "    accuracy                           0.96       999\n",
            "   macro avg       0.96      0.96      0.96       999\n",
            "weighted avg       0.96      0.96      0.96       999\n",
            "\n"
          ]
        }
      ]
    }
  ]
}